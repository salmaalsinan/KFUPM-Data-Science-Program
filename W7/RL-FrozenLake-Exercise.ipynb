{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of possible actions: 4\n"
     ]
    }
   ],
   "source": [
    "# start a new environment \"FrozenLake-v0\"\n",
    "env = gym.make(\"FrozenLake-v0\") \n",
    "# Query how many no. of states\n",
    "n_states = env.observation_space.n \n",
    "# Query how many possible actions\n",
    "n_actions =  env.action_space.n \n",
    "print (\"Number of states: \" + str(n_states))\n",
    "print (\"Number of possible actions: \" + str(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at the setup\n",
    "\n",
    "**===STATE TYPES===**\n",
    "\n",
    "(S: starting point, safe, reward = 0)\n",
    "\n",
    "(F: frozen surface, safe, reward = 0)\n",
    "\n",
    "(H: hole, failed, reward = 0)\n",
    "\n",
    "(G: goal, where the frisbee is located, reward = 1)\n",
    "\n",
    "**===ACTIONS===**\n",
    "\n",
    "(0: Move Left)\n",
    "\n",
    "(1: Move Down)\n",
    "\n",
    "(2: Move Right)\n",
    "\n",
    "(3: Move Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Q-learning**\n",
    "\n",
    "Do the necessary initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize the Q table, rows are #states and columns are #actions\n",
    "## UPDATE CODE BELOW\n",
    "Q_table = None\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some resonably\n",
    "n_episodes = 10000 #number of episodes of learning\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99 #how much we want to penalize for delay\n",
    "epsilon = 0.2 #exploration rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the cummalative rewards per episode to see how we progress\n",
    "rewards_all_episodes = np.zeros((n_episodes)) \n",
    "# Q-learning algorithm\n",
    "for episode in range(n_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    ## UPDATE CODE BELOW\n",
    "    rewards_current_episode = None\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        # With epsilon probability select a random action\n",
    "        # With (1-epsilon) probability select the current optimal action\n",
    "        ## ADD YOUR CODE\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## END ADD YOUR CODE\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # Update Q-table using the Q-Learning approach\n",
    "        ## UPDATE CODE BELOW\n",
    "        Q_table[state, action]= None\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        # Add new reward\n",
    "        ## UPDATE CODE BELOW\n",
    "        rewards_current_episode = None\n",
    "        if done == True: \n",
    "            break\n",
    "        # Exploration rate decay, (later)\n",
    "        \n",
    "    # Add current episode reward to total rewards list\n",
    "    ## UPDATE CODE BELOW\n",
    "    rewards_all_episodes[episode] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),n_episodes/1000)\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent plays the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"====EPISODE \", episode+1, \"====\\n\\n\\n\\n\")\n",
    "    print(\"=============================\\n\")\n",
    "    time.sleep(1)    \n",
    "    \n",
    "    for step in range(100):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Select the best action at the current state\n",
    "        ## UPDATE CODE BELOW\n",
    "        action = None        \n",
    "        # Take the action\n",
    "        ## UPDATE CODE BELOW\n",
    "        new_state, reward, done, info = None\n",
    "        \n",
    "        #print(reward, done)\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            \n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        # Update the current state\n",
    "        ## UPDATE CODE BELOW\n",
    "        state = None\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things for you to try\n",
    "\n",
    "1. What if the environment was not stochastic, i.e., actions lead you to correct location? Do you still need learning? What changes are needed? You can use \"env = gym.make(\"FrozenLake-v0\", is_slippery=False)\" to make the actions deterministic.\n",
    "\n",
    "2. What if we make Gamma as 1.0? What if we make it too small?\n",
    "\n",
    "3. Instead of epsilon being fixed, change it such that it starts high (~1) and decays with time.\n",
    "4. Try a bigger board 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
