{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 64\n",
      "Number of possible actions: 4\n"
     ]
    }
   ],
   "source": [
    "# start a new environment \"FrozenLake-v0\"\n",
    "env = gym.make(\"FrozenLake8x8-v0\") \n",
    "# Query how many no. of states\n",
    "n_states = env.observation_space.n \n",
    "# Query how many possible actions\n",
    "n_actions =  env.action_space.n \n",
    "print (\"Number of states: \" + str(n_states))\n",
    "print (\"Number of possible actions: \" + str(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at the setup\n",
    "\n",
    "**===STATE TYPES===**\n",
    "\n",
    "(S: starting point, safe, reward = 0)\n",
    "\n",
    "(F: frozen surface, safe, reward = 0)\n",
    "\n",
    "(H: hole, failed, reward = 0)\n",
    "\n",
    "(G: goal, where the frisbee is located, reward = 1)\n",
    "\n",
    "**===ACTIONS===**\n",
    "\n",
    "(0: Move Left)\n",
    "\n",
    "(1: Move Down)\n",
    "\n",
    "(2: Move Right)\n",
    "\n",
    "(3: Move Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Q-learning**\n",
    "\n",
    "Do the necessary initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#intialize the Q table, rows are #states and columns are #actions\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000 #number of episodes of learning\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99 #how much we want to penalize for delay\n",
    "\n",
    "epsilon = 0.2 #exploration rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = np.zeros((n_episodes)) # to store the cummalative rewards per episode to see how we progress\n",
    "# Q-learning algorithm\n",
    "for episode in range(n_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        if(random.uniform(0, 1)>epsilon):\n",
    "            #action = np.argmax(Q_table[state,:]) \n",
    "            max_list = np.argwhere(Q_table[state] == np.max(Q_table[state,:]))\n",
    "            #print(max_list)\n",
    "            if(max_list.shape[0] > 1):\n",
    "                pos = random.randint(0,max_list.shape[0]-1)\n",
    "                action = max_list[pos,0]\n",
    "            else:\n",
    "                action = np.argmax(Q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # Update Q-table\n",
    "        Q_table[state, action]=(1 - learning_rate)*Q_table[state, action] + learning_rate*(reward + gamma*np.max(Q_table[new_state, :]))\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        # Add new reward\n",
    "        rewards_current_episode = rewards_current_episode + reward\n",
    "        if done == True: \n",
    "            break\n",
    "        # Exploration rate decay\n",
    "        \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes[episode] = rewards_current_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.20400000000000015\n",
      "2000 :  0.22600000000000017\n",
      "3000 :  0.21800000000000017\n",
      "4000 :  0.19200000000000014\n",
      "5000 :  0.23400000000000018\n",
      "6000 :  0.21100000000000016\n",
      "7000 :  0.20800000000000016\n",
      "8000 :  0.21900000000000017\n",
      "9000 :  0.22700000000000017\n",
      "10000 :  0.21500000000000016\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),n_episodes/1000)\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[3.79144351e-01 3.80694794e-01 3.88023653e-01 3.79647754e-01]\n",
      " [3.86120523e-01 3.93772026e-01 3.99571011e-01 3.89772878e-01]\n",
      " [4.07306105e-01 4.12288261e-01 4.15244137e-01 4.11947026e-01]\n",
      " [4.25334282e-01 4.33469081e-01 4.39870547e-01 4.35531862e-01]\n",
      " [4.50365207e-01 4.53564484e-01 4.67339357e-01 4.50852859e-01]\n",
      " [4.70972844e-01 4.81828502e-01 4.99151545e-01 4.79800908e-01]\n",
      " [4.99264861e-01 5.03694424e-01 5.24972230e-01 5.04435181e-01]\n",
      " [5.09579614e-01 5.34140821e-01 5.14555865e-01 5.05890318e-01]\n",
      " [3.78357688e-01 3.76741226e-01 3.78364114e-01 3.81872431e-01]\n",
      " [3.80695801e-01 3.84694556e-01 3.82689110e-01 3.91843624e-01]\n",
      " [3.86533146e-01 3.93740108e-01 3.97298630e-01 4.07723026e-01]\n",
      " [3.01865530e-01 2.39181616e-01 2.57987942e-01 4.31341385e-01]\n",
      " [4.27061166e-01 4.35839114e-01 4.25993468e-01 4.69865290e-01]\n",
      " [4.73347209e-01 4.73581491e-01 5.08425578e-01 4.74552685e-01]\n",
      " [5.06172152e-01 5.23637798e-01 5.32199927e-01 5.12436733e-01]\n",
      " [5.29237887e-01 5.60062817e-01 5.35244380e-01 5.19577655e-01]\n",
      " [3.70847907e-01 3.70902496e-01 3.70691301e-01 3.72223148e-01]\n",
      " [3.70480937e-01 3.68907528e-01 3.68724278e-01 3.73292120e-01]\n",
      " [3.52206575e-01 1.91173682e-01 2.28716384e-01 2.32463452e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.01813956e-01 2.48488805e-01 3.75891802e-01 2.40777791e-01]\n",
      " [2.91754133e-01 3.58064169e-01 4.04403559e-01 4.80002549e-01]\n",
      " [5.04758633e-01 5.18997894e-01 5.53710216e-01 5.04744292e-01]\n",
      " [5.78714357e-01 5.97941193e-01 5.70176012e-01 5.58972719e-01]\n",
      " [3.55829378e-01 3.44422886e-01 3.57669120e-01 3.57737017e-01]\n",
      " [3.34816851e-01 3.31097998e-01 3.25773486e-01 3.41561232e-01]\n",
      " [2.84915542e-01 2.46657569e-01 2.46063584e-01 2.89778596e-01]\n",
      " [1.08958864e-01 1.73759221e-01 6.20676396e-02 1.84415219e-01]\n",
      " [2.70803893e-01 1.22418321e-01 1.76703828e-01 1.40744955e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.65157449e-01 4.43945783e-01 5.47623091e-01 3.31427926e-01]\n",
      " [6.19357940e-01 6.24004884e-01 6.58818552e-01 5.73661560e-01]\n",
      " [1.87262850e-01 1.44555024e-01 1.82282192e-01 3.38885567e-01]\n",
      " [1.99655729e-01 1.96726715e-01 2.41440335e-01 3.05590421e-01]\n",
      " [2.41671432e-01 5.40437326e-02 2.06422381e-02 1.99456630e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.76358411e-01 1.54822630e-01 2.26386097e-01 1.77928741e-01]\n",
      " [1.49081661e-01 2.36173843e-01 2.37117258e-01 2.74138047e-01]\n",
      " [2.89964501e-01 4.24200482e-01 3.65266021e-01 4.36447844e-01]\n",
      " [6.17224369e-01 6.74175257e-01 7.04401069e-01 6.00680378e-01]\n",
      " [4.58798765e-06 0.00000000e+00 1.34416378e-01 1.12288283e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.72687112e-03 5.33548751e-02 4.31617422e-02 4.20707161e-02]\n",
      " [7.03429882e-02 5.75824607e-02 1.20227467e-01 1.19491615e-01]\n",
      " [2.07843509e-01 1.13258700e-01 1.41610429e-01 1.65982557e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.21320740e-01 6.64087027e-01 8.33994809e-01 6.50681327e-01]\n",
      " [0.00000000e+00 0.00000000e+00 3.97236120e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.63985042e-03 0.00000000e+00 1.64462149e-03]\n",
      " [1.80908364e-04 3.19358938e-04 1.43517938e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.01834752e-01 8.21380004e-02 1.71394985e-01 7.48419180e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.44271940e-01 6.14126423e-01 9.16850178e-01 5.09744772e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.69519536e-02 1.31457231e-01 0.00000000e+00 0.00000000e+00]\n",
      " [1.28595221e-01 2.47847893e-01 2.92833354e-01 1.77652134e-01]\n",
      " [1.41007417e-01 3.13807619e-01 1.68893033e-01 3.96144033e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent plays the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"====EPISODE \", episode+1, \"====\\n\\n\\n\\n\")\n",
    "    print(\"=============================\\n\")\n",
    "    time.sleep(1)    \n",
    "    \n",
    "    for step in range(100):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(Q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #print(reward, done)\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            \n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times success: 64\n"
     ]
    }
   ],
   "source": [
    "# Check how many times it reached the goal in 100 attempts\n",
    "n_attempts = 100\n",
    "n_success = 0\n",
    "for episode in range(n_attempts):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "   \n",
    "    for step in range(100):        \n",
    "        \n",
    "        action = np.argmax(Q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                n_success +=1\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print ('times success: ' + str(n_success))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things for you to try\n",
    "\n",
    "1. What if the environment was not stochastic, i.e., actions lead you to correct location? Do you still need learning? What changes are needed? You can use \"env = gym.make(\"FrozenLake-v0\", is_slippery=False)\" to make the actions deterministic\n",
    "\n",
    "2. What if we make Gamma as 1.0? What if we make it too small?\n",
    "\n",
    "3. Instead of epsilon being fixed, change it such that it starts high (~1) and decays with time.\n",
    "4. Try a bigger board 'FrozenLake8x8-v0'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
